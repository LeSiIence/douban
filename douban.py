import requests
from bs4 import BeautifulSoup
import time
import random
import csv
import os
import argparse

# å°è¯•å¯¼å…¥Seleniumï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# --- é…ç½® ---
# ç›®æ ‡ç½‘å€ï¼ˆä½¿ç”¨çƒ­åº¦æ’åºï¼‰
BASE_URL = "https://read.douban.com/category/105?sort=hot"
# è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
# å›¾ç‰‡å­˜å‚¨ç›®å½•
IMAGE_DIR = 'images'
# CSVæ–‡ä»¶è·¯å¾„
CSV_FILE = 'books.csv'
# çˆ¬å–é¡µæ•°
MAX_PAGES = 3  # é»˜è®¤çˆ¬å–3é¡µ

# --- è°ƒè¯•é…ç½® ---
DEBUG_MODE = True  # è®¾ç½®ä¸ºFalseå¯å…³é—­æ‰€æœ‰è°ƒè¯•è¾“å‡º
SAVE_HTML = True   # æ˜¯å¦ä¿å­˜åŸå§‹HTMLæ–‡ä»¶ç”¨äºè°ƒè¯•
USE_SELENIUM = SELENIUM_AVAILABLE  # å¦‚æœSeleniumå¯ç”¨åˆ™é»˜è®¤ä½¿ç”¨

# --- è°ƒè¯•å·¥å…·å‡½æ•° ---
def debug_print(message, level="INFO"):
    """
    è°ƒè¯•è¾“å‡ºå‡½æ•°
    level: INFO, ERROR, SUCCESS
    """
    if DEBUG_MODE:
        prefix_map = {
            "INFO": "[è°ƒè¯•]",
            "ERROR": "[é”™è¯¯]", 
            "SUCCESS": "[æˆåŠŸ]",
            "CONFIG": "[é…ç½®]",
            "MAIN": "[ä¸»ç¨‹åº]"
        }
        prefix = prefix_map.get(level, "[è°ƒè¯•]")
        print(f"{prefix} {message}")

def create_selenium_driver():
    """
    åˆ›å»ºSelenium WebDriver
    """
    if not SELENIUM_AVAILABLE:
        return None
    
    try:
        options = Options()
        options.add_argument('--headless')  # æ— å¤´æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        # å°è¯•åˆ›å»ºWebDriver
        driver = webdriver.Chrome(options=options)
        debug_print("æˆåŠŸåˆ›å»ºSelenium WebDriver", "SUCCESS")
        return driver
    except Exception as e:
        debug_print(f"åˆ›å»ºSelenium WebDriverå¤±è´¥: {e}", "ERROR")
        debug_print("å°†å›é€€åˆ°æ™®é€šrequestsæ–¹æ¡ˆ", "INFO")
        return None

def fetch_page_with_selenium(driver, url):
    """
    ä½¿ç”¨Seleniumè·å–é¡µé¢å†…å®¹
    """
    try:
        debug_print(f"ä½¿ç”¨SeleniumåŠ è½½é¡µé¢: {url}")
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
        wait = WebDriverWait(driver, 10)
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "works-list")))
        
        # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ
        time.sleep(2)
        
        # å°è¯•æ»šåŠ¨é¡µé¢ï¼Œè§¦å‘æ‡’åŠ è½½
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)
        
        # è·å–é¡µé¢æºç 
        html_content = driver.page_source
        debug_print(f"é€šè¿‡Seleniumè·å–åˆ°é¡µé¢å†…å®¹ï¼Œé•¿åº¦: {len(html_content)}", "SUCCESS")
        return html_content
        
    except TimeoutException:
        debug_print("é¡µé¢åŠ è½½è¶…æ—¶", "ERROR")
        return None
    except Exception as e:
        debug_print(f"Seleniumè·å–é¡µé¢å¤±è´¥: {e}", "ERROR")
        return None

# --- åˆå§‹åŒ– ---
# åˆ›å»ºå›¾ç‰‡å­˜å‚¨ç›®å½•
if not os.path.exists(IMAGE_DIR):
    os.makedirs(IMAGE_DIR)

# æ‰“å¼€CSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´ï¼ˆä½¿ç”¨UTF-8 BOMç¼–ç ï¼Œç¡®ä¿Officeæ­£ç¡®æ˜¾ç¤ºä¸­æ–‡ï¼‰
with open(CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
    writer = csv.writer(f)
    writer.writerow(['çƒ­åº¦æ’å', 'ä¹¦å', 'ä½œè€…', 'ç®€ä»‹', 'åˆ†ç±»', 'å­—æ•°', 'ä»·æ ¼'])

# --- çˆ¬è™«æ ¸å¿ƒé€»è¾‘ ---

def init_webdriver():
    """åˆå§‹åŒ–Chrome WebDriver"""
    if not SELENIUM_AVAILABLE:
        debug_print("âŒ Seleniumä¸å¯ç”¨ï¼Œæ— æ³•åˆå§‹åŒ–WebDriver", "ERROR")
        return None
    
    try:
        debug_print("æ­£åœ¨åˆå§‹åŒ–Chrome WebDriver...")
        
        # é…ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # æ— ç•Œé¢æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        # è‡ªåŠ¨ä¸‹è½½å’Œé…ç½®ChromeDriver
        service = Service(ChromeDriverManager().install())
        
        # åˆ›å»ºWebDriverå®ä¾‹
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        debug_print("âœ… Chrome WebDriveråˆå§‹åŒ–æˆåŠŸ")
        return driver
        
    except Exception as e:
        debug_print(f"âŒ åˆå§‹åŒ–WebDriverå¤±è´¥: {e}", "ERROR")
        return None

def fetch_book_data_selenium(url, driver, page_num=1):
    """
    ä½¿ç”¨Seleniumè·å–ä¹¦ç±æ•°æ®ï¼ˆå¤„ç†JavaScriptåŠ¨æ€åŠ è½½ï¼‰
    """
    debug_print(f"å¼€å§‹è¯·æ±‚ç¬¬{page_num}é¡µURL: {url}")
    debug_print("ä½¿ç”¨Seleniumæ¨¡å¼è·å–é¡µé¢")
    
    try:
        # ä½¿ç”¨Seleniumè·å–é¡µé¢
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ - å…ˆç­‰å¾…works-listå®¹å™¨
        wait = WebDriverWait(driver, 10)
        works_list = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "ul.works-list"))
        )
        
        # ç­‰å¾…å®é™…çš„ä¹¦ç±æ•°æ®åŠ è½½ï¼ˆè€Œéloading skeletonï¼‰
        debug_print("ç­‰å¾…å®é™…ä¹¦ç±æ•°æ®åŠ è½½...")
        try:
            # ç­‰å¾…è‡³å°‘æœ‰ä¸€ä¸ªå¸¦æœ‰data-works-idå±æ€§çš„ä¹¦ç±é¡¹ç›®å‡ºç°
            wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "li[data-works-id]"))
            )
            # é¢å¤–ç­‰å¾…ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½åŠ è½½å®Œæˆ
            time.sleep(3)
            debug_print("æ£€æµ‹åˆ°å®é™…ä¹¦ç±æ•°æ®å·²åŠ è½½")
        except TimeoutException:
            debug_print("æœªæ£€æµ‹åˆ°å®é™…ä¹¦ç±æ•°æ®ï¼Œå¯èƒ½é¡µé¢ä»åœ¨åŠ è½½ä¸­...")
            # ç»™äºˆæ›´å¤šæ—¶é—´
            time.sleep(5)
        
        debug_print("é¡µé¢åŠ è½½å®Œæˆï¼Œå¼€å§‹è§£æ...")
        
        # è·å–é¡µé¢æºç 
        html_content = driver.page_source
        
        # ä¿å­˜HTMLæ–‡ä»¶ç”¨äºè°ƒè¯•
        if SAVE_HTML:
            filename = f"debug_response_selenium_page{page_num}.html"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            debug_print(f"å·²ä¿å­˜åŸå§‹HTMLåˆ° {filename}")
        
        # ä½¿ç”¨BeautifulSoupè§£æHTML
        soup = BeautifulSoup(html_content, 'html.parser')
        page_title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
        debug_print(f"é¡µé¢æ ‡é¢˜: {page_title}")
        
        # æŸ¥æ‰¾ä¹¦ç±åˆ—è¡¨ - ä¼˜å…ˆæŸ¥æ‰¾Reactæ ¹èŠ‚ç‚¹ä¸‹çš„åŠ¨æ€åŠ è½½å†…å®¹
        debug_print("å°è¯•æŸ¥æ‰¾è±†ç“£é¡µé¢ä¸­çš„ä¹¦ç±åˆ—è¡¨...")
        
        # å…ˆå°è¯•æ‰¾Reactæ¸²æŸ“çš„åŠ¨æ€å†…å®¹
        react_root = soup.find('div', id='react-root')
        if react_root:
            works_list = react_root.find('ul', class_='works-list')
            if works_list:
                debug_print("âœ… æ‰¾åˆ°ReactåŠ¨æ€åŠ è½½çš„works-listå®¹å™¨")
            else:
                debug_print("âš ï¸  Reactæ ¹èŠ‚ç‚¹å­˜åœ¨ä½†æœªæ‰¾åˆ°works-list")
                works_list = None
        else:
            debug_print("âš ï¸  æœªæ‰¾åˆ°Reactæ ¹èŠ‚ç‚¹")
            works_list = None
        
        # å¦‚æœåŠ¨æ€å†…å®¹æ²¡æ‰¾åˆ°ï¼Œå°è¯•é™æ€å†…å®¹
        if not works_list:
            works_list = soup.find('ul', class_='works-list')
            if works_list:
                debug_print("âœ… æ‰¾åˆ°é™æ€works-listå®¹å™¨")
            else:
                debug_print("âŒ æœªæ‰¾åˆ°ä»»ä½•works-listå®¹å™¨")
                return []
        
        # è·å–æ‰€æœ‰å®é™…çš„ä¹¦ç±é¡¹ç›®ï¼ˆæœ‰data-works-idçš„ï¼Œæ’é™¤loading skeletonï¼‰
        book_items = works_list.find_all('li', {'data-works-id': True})
        debug_print(f"åœ¨works-listä¸­æ‰¾åˆ° {len(book_items)} ä¸ªå®é™…ä¹¦ç±liå…ƒç´ ")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®é™…ä¹¦ç±ï¼Œæ£€æŸ¥loadingçŠ¶æ€
        if not book_items:
            loading_items = works_list.find_all('li', class_='works-item is-loading')
            debug_print(f"å‘ç° {len(loading_items)} ä¸ªloadingé¡¹ç›®ï¼Œæ•°æ®å¯èƒ½è¿˜åœ¨åŠ è½½ä¸­")
            all_li_items = works_list.find_all('li')
            debug_print(f"æ€»å…±æœ‰ {len(all_li_items)} ä¸ªliå…ƒç´ ")
            return []
        
        books_data = []
        
        for i, book in enumerate(book_items):
            debug_print(f"å¤„ç†ç¬¬ {i+1} æœ¬ä¹¦...")
            try:
                # åŸºäºè±†ç“£å®é™…HTMLç»“æ„æå–ä¿¡æ¯
                # æå–æ ‡é¢˜
                title_elem = book.find('h4', class_='title')
                if title_elem:
                    title_link = title_elem.find('a')
                    if title_link:
                        title_span = title_link.find('span', class_='title-text')
                        if title_span:
                            title = title_span.get_text(strip=True)
                        else:
                            title = title_link.get_text(strip=True)
                    else:
                        title = title_elem.get_text(strip=True)
                    debug_print(f"ä¹¦å: {title}")
                else:
                    debug_print(f"ç¬¬ {i+1} æœ¬ä¹¦ç¼ºå°‘æ ‡é¢˜ä¿¡æ¯")
                    continue
                
                # æå–ä½œè€…ä¿¡æ¯
                author_elem = book.find('div', class_='author')
                if author_elem:
                    author_links = author_elem.find_all('a', class_='author-link')
                    if author_links:
                        authors = [link.get_text(strip=True) for link in author_links]
                        author = ' / '.join(authors)
                    else:
                        author = author_elem.get_text(strip=True)
                    debug_print(f"ä½œè€…: {author}")
                else:
                    author = "æœªçŸ¥ä½œè€…"
                    debug_print(f"ç¬¬ {i+1} æœ¬ä¹¦ç¼ºå°‘ä½œè€…ä¿¡æ¯")
                
                # æå–ç®€ä»‹ä¿¡æ¯
                intro_elem = book.find('a', class_='intro')
                if intro_elem:
                    intro = intro_elem.get_text(strip=True)
                    if len(intro) > 200:
                        intro = intro[:200] + "..."
                    debug_print(f"ç®€ä»‹é•¿åº¦: {len(intro)} å­—ç¬¦")
                else:
                    intro = "æ— ç®€ä»‹ä¿¡æ¯"
                    debug_print(f"ç¬¬ {i+1} æœ¬ä¹¦ç¼ºå°‘ç®€ä»‹ä¿¡æ¯")
                
                # æå–å­—æ•°ä¿¡æ¯
                word_count = "æœªçŸ¥"
                extra_info = book.find('div', class_='extra-info')
                if extra_info:
                    spans = extra_info.find_all('span')
                    for span in spans:
                        text = span.get_text(strip=True)
                        if 'ä¸‡å­—' in text:
                            word_count = text
                            break
                
                # æå–ä»·æ ¼ä¿¡æ¯
                price = "æœªçŸ¥"
                price_elem = book.find('span', class_='price-tag')
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    price = price_text if price_text else "æœªçŸ¥"
                
                # è®¡ç®—çƒ­åº¦æ’å
                ranking = (page_num - 1) * 10 + (i + 1)
                
                # ç»„è£…æ•°æ®
                data_row = {
                    'çƒ­åº¦æ’å': ranking,
                    'ä¹¦å': title,
                    'ä½œè€…': author,
                    'ç®€ä»‹': intro,
                    'å­—æ•°': word_count,
                    'ä»·æ ¼': price
                }
                books_data.append(data_row)
                debug_print(f"æˆåŠŸæå–ç¬¬ {i+1} æœ¬ä¹¦çš„ä¿¡æ¯", "SUCCESS")
                
            except Exception as e:
                debug_print(f"å¤„ç†ç¬¬ {i+1} æœ¬ä¹¦æ—¶å‡ºé”™: {e}", "ERROR")
        
        debug_print(f"æˆåŠŸæå– {len(books_data)} æœ¬ä¹¦çš„æ•°æ®", "SUCCESS")
        return books_data
        
    except TimeoutException:
        debug_print("âŒ é¡µé¢åŠ è½½è¶…æ—¶", "ERROR")
        return None
    except WebDriverException as e:
        debug_print(f"âŒ WebDriveré”™è¯¯: {e}", "ERROR")
        return None
    except Exception as e:
        debug_print(f"âŒ å…¶ä»–é”™è¯¯: {e}", "ERROR")
        return None

def fetch_book_data(url, page_num=1, start_rank=1, driver=None):
    """
    çˆ¬å–æŒ‡å®šURLçš„é¡µé¢å¹¶æå–ä¹¦ç±ä¿¡æ¯
    page_num: é¡µç ï¼ˆç”¨äºè°ƒè¯•æ˜¾ç¤ºï¼‰
    start_rank: èµ·å§‹æ’åï¼ˆç”¨äºè®¡ç®—çƒ­åº¦æ’åï¼‰
    driver: Selenium WebDriverå®ä¾‹ï¼ˆå¯é€‰ï¼‰
    """
    try:
        debug_print(f"å¼€å§‹è¯·æ±‚ç¬¬{page_num}é¡µURL: {url}")
        
        # æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨Seleniumè¿˜æ˜¯requests
        if USE_SELENIUM and SELENIUM_AVAILABLE and driver:
            debug_print("ä½¿ç”¨Seleniumæ¨¡å¼è·å–é¡µé¢")
            html_content = fetch_page_with_selenium(driver, url)
            if html_content is None:
                debug_print("Seleniumè·å–å¤±è´¥ï¼Œå›é€€åˆ°requestsæ¨¡å¼", "ERROR")
                # å›é€€åˆ°requests
                response = requests.get(url, headers=HEADERS)
                html_content = response.text
            else:
                # ä¿å­˜Seleniumè·å–çš„HTML
                if SAVE_HTML:
                    filename = f'debug_selenium_page{page_num}.html'
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    debug_print(f"å·²ä¿å­˜Selenium HTMLåˆ° {filename}")
        else:
            debug_print("ä½¿ç”¨requestsæ¨¡å¼è·å–é¡µé¢")
            # ä»£ç†IPï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
            # proxies = { "http": "http://your_proxy_ip:port", "https": "https://your_proxy_ip:port" }
            # response = requests.get(url, headers=HEADERS, proxies=proxies)

            response = requests.get(url, headers=HEADERS)
            debug_print(f"HTTPçŠ¶æ€ç : {response.status_code}")
            debug_print(f"å“åº”å¤´: {dict(response.headers)}")
            
            response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸
            html_content = response.text

            # ä¿å­˜åŸå§‹HTMLç”¨äºè°ƒè¯•
            debug_print(f"å“åº”å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
            if SAVE_HTML:
                filename = f'debug_response_page{page_num}.html'
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(response.text)
                debug_print(f"å·²ä¿å­˜åŸå§‹HTMLåˆ° {filename}")

        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æ‰“å°é¡µé¢æ ‡é¢˜ç”¨äºç¡®è®¤
        title_tag = soup.find('title')
        if title_tag:
            debug_print(f"é¡µé¢æ ‡é¢˜: {title_tag.text}")
        
        # æ ¹æ®å®é™…é¡µé¢ç»“æ„æŸ¥æ‰¾ä¹¦ç±åˆ—è¡¨
        debug_print("å°è¯•æŸ¥æ‰¾è±†ç“£é¡µé¢ä¸­çš„ä¹¦ç±åˆ—è¡¨...")
        
        # æŸ¥æ‰¾works-listå®¹å™¨
        works_list = soup.find('ul', class_='works-list')
        if works_list:
            debug_print("æ‰¾åˆ°works-listå®¹å™¨")
            books = works_list.find_all('li')
            debug_print(f"åœ¨works-listä¸­æ‰¾åˆ° {len(books)} ä¸ªliå…ƒç´ ")
        else:
            debug_print("æœªæ‰¾åˆ°works-listå®¹å™¨ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾liå…ƒç´ ...")
            books = soup.find_all('li')
            debug_print(f"ç›´æ¥æ‰¾åˆ° {len(books)} ä¸ªliå…ƒç´ ")
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å¸¸è§çš„é€‰æ‹©å™¨
        if len(books) == 0:
            debug_print("å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨...")
            
            # å°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"ä¹¦"å­—çš„div
            book_divs = soup.find_all('div', string=lambda text: text and 'ä¹¦' in text)
            debug_print(f"æ‰¾åˆ°åŒ…å«'ä¹¦'å­—çš„div: {len(book_divs)} ä¸ª")
            
            # æŸ¥æ‰¾æ‰€æœ‰classåŒ…å«bookçš„å…ƒç´ 
            book_elements = soup.find_all(class_=lambda x: x and 'book' in x.lower())
            debug_print(f"æ‰¾åˆ°classåŒ…å«'book'çš„å…ƒç´ : {len(book_elements)} ä¸ª")
            for elem in book_elements[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                debug_print(f"å…ƒç´ : {elem.name}, class: {elem.get('class')}")
            
            # æŸ¥æ‰¾æ‰€æœ‰imgæ ‡ç­¾
            img_tags = soup.find_all('img')
            debug_print(f"æ‰¾åˆ°å›¾ç‰‡æ ‡ç­¾: {len(img_tags)} ä¸ª")
            
            # æŸ¥æ‰¾æ‰€æœ‰aæ ‡ç­¾  
            a_tags = soup.find_all('a')
            debug_print(f"æ‰¾åˆ°é“¾æ¥æ ‡ç­¾: {len(a_tags)} ä¸ª")
        
        book_data = []
        for i, book in enumerate(books):
            current_rank = start_rank + i  # è®¡ç®—å½“å‰ä¹¦ç±çš„çƒ­åº¦æ’å
            debug_print(f"å¤„ç†ç¬¬ {i+1} æœ¬ä¹¦ï¼ˆæ’åç¬¬{current_rank}ï¼‰...")
            
            try:
                # æ ¹æ®å®é™…HTMLç»“æ„æå–ä¿¡æ¯
                title_elem = book.find('div', class_='title')
                author_elem = book.find('div', class_='author')
                abstract_elem = book.find('div', class_='abstract')
                
                if title_elem:
                    title = title_elem.text.strip()
                    debug_print(f"ä¹¦å: {title}")
                else:
                    debug_print(f"ç¬¬ {i+1} æœ¬ä¹¦ç¼ºå°‘æ ‡é¢˜ä¿¡æ¯")
                    continue
                
                # å¤„ç†ä½œè€…ä¿¡æ¯
                if author_elem:
                    # ä½œè€…ä¿¡æ¯å¯èƒ½åŒ…å«å¤šä¸ªaæ ‡ç­¾
                    authors = []
                    author_links = author_elem.find_all('a')
                    if author_links:
                        for link in author_links:
                            authors.append(link.text.strip())
                        author = ', '.join(authors)
                    else:
                        author = author_elem.text.strip()
                    debug_print(f"ä½œè€…: {author}")
                else:
                    author = "æœªçŸ¥ä½œè€…"
                    debug_print(f"ç¬¬ {i+1} æœ¬ä¹¦ç¼ºå°‘ä½œè€…ä¿¡æ¯")
                
                # å¤„ç†ç®€ä»‹ä¿¡æ¯
                if abstract_elem:
                    abstract = abstract_elem.text.strip()
                    # é™åˆ¶ç®€ä»‹é•¿åº¦ç”¨äºæ˜¾ç¤º
                    if len(abstract) > 200:
                        abstract = abstract[:200] + "..."
                    debug_print(f"ç®€ä»‹é•¿åº¦: {len(abstract)} å­—ç¬¦")
                else:
                    abstract = "æ— ç®€ä»‹ä¿¡æ¯"
                    debug_print(f"ç¬¬ {i+1} æœ¬ä¹¦ç¼ºå°‘ç®€ä»‹ä¿¡æ¯")
                
                # ç»„è£…æ•°æ®ï¼ˆå¢åŠ çƒ­åº¦æ’åå­—æ®µï¼‰
                data_row = {
                    'çƒ­åº¦æ’å': current_rank,
                    'ä¹¦å': title,
                    'ä½œè€…': author,
                    'ç®€ä»‹': abstract,
                    'åˆ†ç±»': 'è®¡ç®—æœºä¸äº’è”ç½‘',
                    'å­—æ•°': 'æœªçŸ¥',
                    'ä»·æ ¼': 'æœªçŸ¥'
                }
                book_data.append(data_row)
                debug_print(f"æˆåŠŸæå–ç¬¬ {i+1} æœ¬ä¹¦çš„ä¿¡æ¯ï¼ˆæ’åç¬¬{current_rank}ï¼‰", "SUCCESS")
                
                # ä¸‹è½½å›¾ç‰‡é€»è¾‘ï¼ˆæš‚æ—¶æ³¨é‡Šï¼‰
                # img_elem = book.find('img')
                # if img_elem and img_elem.get('src'):
                #     img_url = img_elem['src']
                #     download_image(img_url, f"{len(book_data)}.jpg")
                
            except Exception as e:
                debug_print(f"å¤„ç†ç¬¬ {i+1} æœ¬ä¹¦æ—¶å‡ºé”™: {e}", "ERROR")
                if DEBUG_MODE:
                    import traceback
                    traceback.print_exc()

        debug_print(f"æˆåŠŸæå– {len(book_data)} æœ¬ä¹¦çš„æ•°æ®", "SUCCESS")
        return book_data

    except requests.exceptions.RequestException as e:
        debug_print(f"è¯·æ±‚å¤±è´¥: {e}", "ERROR")
        return []
    except Exception as e:
        debug_print(f"è§£æé¡µé¢æ—¶å‡ºé”™: {e}", "ERROR")
        return []

def download_image(url, filename):
    """
    ä¸‹è½½å•å¼ å›¾ç‰‡
    """
    try:
        img_response = requests.get(url, headers=HEADERS)
        img_response.raise_for_status()
        with open(os.path.join(IMAGE_DIR, filename), 'wb') as f:
            f.write(img_response.content)
        debug_print(f"å·²ä¸‹è½½: {filename}", "SUCCESS")
    except requests.exceptions.RequestException as e:
        debug_print(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {e}", "ERROR")

def save_to_csv(data):
    """
    å°†æ•°æ®è¿½åŠ åˆ°CSVæ–‡ä»¶ï¼ˆä½¿ç”¨UTF-8 BOMç¼–ç ï¼‰
    """
    with open(CSV_FILE, 'a', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=['çƒ­åº¦æ’å', 'ä¹¦å', 'ä½œè€…', 'ç®€ä»‹', 'åˆ†ç±»', 'å­—æ•°', 'ä»·æ ¼'])
        writer.writerows(data)

# --- ä¸»ç¨‹åº ---
def main():
    """ä¸»å‡½æ•°"""
    global DEBUG_MODE, SAVE_HTML, MAX_PAGES, USE_SELENIUM
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è±†ç“£è¯»ä¹¦çˆ¬è™«')
    parser.add_argument('--debug', action='store_true', help='å¼€å¯è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--no-debug', action='store_true', help='å…³é—­è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--save-html', action='store_true', help='ä¿å­˜åŸå§‹HTMLæ–‡ä»¶')
    parser.add_argument('--no-save-html', action='store_true', help='ä¸ä¿å­˜åŸå§‹HTMLæ–‡ä»¶')
    parser.add_argument('--pages', type=int, default=MAX_PAGES, help=f'çˆ¬å–é¡µæ•° (é»˜è®¤: {MAX_PAGES})')
    parser.add_argument('--use-selenium', action='store_true', help='å¼ºåˆ¶ä½¿ç”¨Seleniumæµè§ˆå™¨æ¨¡å¼')
    parser.add_argument('--no-selenium', action='store_true', help='å¼ºåˆ¶ä½¿ç”¨requestsæ¨¡å¼')
    
    args = parser.parse_args()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è°ƒæ•´é…ç½®
    if args.debug:
        DEBUG_MODE = True
    elif args.no_debug:
        DEBUG_MODE = False
    
    if args.save_html:
        SAVE_HTML = True
    elif args.no_save_html:
        SAVE_HTML = False
    
    if args.use_selenium:
        USE_SELENIUM = True
    elif args.no_selenium:
        USE_SELENIUM = False
    
    MAX_PAGES = args.pages
    
    print("=" * 50)
    print("è±†ç“£è¯»ä¹¦çˆ¬è™«å¯åŠ¨")
    print("=" * 50)
    
    debug_print(f"ç¨‹åºé…ç½®:")
    debug_print(f"- è°ƒè¯•æ¨¡å¼: {DEBUG_MODE}")
    debug_print(f"- ä¿å­˜HTML: {SAVE_HTML}")
    debug_print(f"- çˆ¬å–é¡µæ•°: {MAX_PAGES}")
    debug_print(f"- ä½¿ç”¨Selenium: {USE_SELENIUM}")
    debug_print(f"- Seleniumå¯ç”¨: {SELENIUM_AVAILABLE}")
    
    # æ ¹æ®é…ç½®é€‰æ‹©çˆ¬å–æ–¹å¼
    if USE_SELENIUM:
        if not SELENIUM_AVAILABLE:
            print("âŒ é”™è¯¯ï¼šæŒ‡å®šä½¿ç”¨Seleniumä½†Seleniumä¸å¯ç”¨")
            return
        debug_print("âœ… å°†ä½¿ç”¨Seleniumæµè§ˆå™¨æ¨¡å¼è¿›è¡Œçˆ¬å–")
    else:
        debug_print("ğŸ”§ å°†ä½¿ç”¨requestsæ¨¡å¼è¿›è¡Œçˆ¬å–")
    
    all_books = []
    current_ranking = 1
    driver = None
    
    # åˆå§‹åŒ–WebDriverï¼ˆå¦‚æœéœ€è¦ï¼‰
    if USE_SELENIUM and SELENIUM_AVAILABLE:
        debug_print("åˆå§‹åŒ–Chromeæµè§ˆå™¨...")
        driver = init_webdriver()
        if driver is None:
            print("âŒ é”™è¯¯ï¼šæ— æ³•åˆå§‹åŒ–WebDriver")
            return
    
    try:
        debug_print("å¼€å§‹çˆ¬å–è±†ç“£è¯»ä¹¦...")
        
        for page_num in range(1, MAX_PAGES + 1):
            debug_print(f"\n--- å¼€å§‹çˆ¬å–ç¬¬ {page_num} é¡µ ---")
            
            url = f"https://read.douban.com/category/105?sort=hot&page={page_num}"
            
            # æ ¹æ®é…ç½®é€‰æ‹©çˆ¬å–æ–¹å¼
            if USE_SELENIUM and driver:
                books_data = fetch_book_data_selenium(url, driver, page_num)
            else:
                books_data = fetch_book_data(url, page_num)
            
            if books_data is None:
                debug_print(f"ç¬¬ {page_num} é¡µè·å–å¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            if not books_data:
                debug_print(f"ç¬¬ {page_num} é¡µæ²¡æœ‰æ‰¾åˆ°ä¹¦ç±æ•°æ®")
                continue
            
            # æ·»åŠ æ’åä¿¡æ¯
            for book in books_data:
                book['çƒ­åº¦æ’å'] = current_ranking
                current_ranking += 1
            
            all_books.extend(books_data)
            
            debug_print(f"ç¬¬ {page_num} é¡µæˆåŠŸè·å– {len(books_data)} æœ¬ä¹¦")
            
            # å»¶è¿Ÿé¿å…è¯·æ±‚å¤ªé¢‘ç¹
            if page_num < MAX_PAGES:
                debug_print("ç­‰å¾…2ç§’...")
                time.sleep(2)
    
    finally:
        # å…³é—­WebDriver
        if driver:
            debug_print("å…³é—­æµè§ˆå™¨...")
            driver.quit()
    
    debug_print(f"æ‰€æœ‰é¡µé¢çˆ¬å–å®Œæˆï¼Œå…±è·å¾— {len(all_books)} æœ¬ä¹¦çš„æ•°æ®")
    
    if all_books:
        debug_print("å¼€å§‹ä¿å­˜æ•°æ®åˆ°CSV...")
        save_to_csv(all_books)
        debug_print(f"æˆåŠŸå°† {len(all_books)} æœ¬ä¹¦çš„ä¿¡æ¯å­˜å…¥ {CSV_FILE}")
    else:
        debug_print("âš ï¸  æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®ï¼")
        debug_print("è¯·æ£€æŸ¥:")
        debug_print("  1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        debug_print("  2. ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯ä»¥è®¿é—®")
        debug_print("  3. CSSé€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®")
        debug_print("  4. æ˜¯å¦éœ€è¦å¤„ç†åçˆ¬è™«æœºåˆ¶")
        debug_print("  5. æ£€æŸ¥ debug_response_page*.html æ–‡ä»¶æŸ¥çœ‹å®é™…é¡µé¢å†…å®¹")

    debug_print("ç¨‹åºç»“æŸ", "MAIN")

if __name__ == "__main__":
    main()